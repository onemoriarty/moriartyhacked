import requests
import re
import os
import gzip
import threading
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import warnings
import sys

warnings.filterwarnings('ignore', message='Unverified HTTPS request')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ğŸ¯ ULTRA HIGH-PERFORMANCE JUGGERNAUT CONFIGURATION (Legacy Compatible)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASE_URL = "https://commoncrawl.org/get-started"
OUTPUT_DIR = "gzfiles"

# ğŸ”¥ PERFORMANCE TUNING
MAX_WORKERS = 100
PARALLEL_ARCHIVE_SCAN = 50
CHUNK_SIZE = 1024 * 1024 # 1 MB
CONNECT_TIMEOUT = 15
READ_TIMEOUT = 300
MAX_RETRIES = 5
BACKOFF_FACTOR = 0.5
BATCH_SIZE = 1000

# ğŸ¨ RENK KODLARI
GREEN = '\033[92m'
YELLOW = '\033[93m'
CYAN = '\033[96m'
RED = '\033[91m'
MAGENTA = '\033[95m'
BOLD = '\033[1m'
ENDC = '\033[0m'

thread_local = threading.local()

def create_optimized_session():
    """Her thread iÃ§in Ã¶zel, optimize edilmiÅŸ ve Python 3.6 uyumlu retry mekanizmalÄ± session oluÅŸturur."""
    if not hasattr(thread_local, "session"):
        session = requests.Session()
        
        # --- PYTHON 3.6 UYUMLULUK DÃœZELTMESÄ° ---
        # `allowed_methods` yerine, eski versiyonlarÄ±n anladÄ±ÄŸÄ± `method_whitelist` kullanÄ±lÄ±r.
        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=BACKOFF_FACTOR,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET"]  # 'allowed_methods' yerine bu kullanÄ±lÄ±r.
        )
        # -----------------------------------------
        
        adapter = HTTPAdapter(
            pool_connections=MAX_WORKERS,
            pool_maxsize=MAX_WORKERS * 2,
            max_retries=retry_strategy
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update({
            'User-Agent': f'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        thread_local.session = session
    return thread_local.session

# tqdm kÃ¼tÃ¼phanesi olmadan Ã§alÄ±ÅŸan basit bir ilerleme Ã§ubuÄŸu sÄ±nÄ±fÄ±
class SimpleProgress:
    def __init__(self, total, desc="Progress"):
        self.total = total
        self.current = 0
        self.desc = desc
        self.lock = threading.Lock()
        self.start_time = time.time()

    def update(self, n=1):
        with self.lock:
            self.current += n
            elapsed = time.time() - self.start_time
            rate = self.current / elapsed if elapsed > 0 else 0
            percent = (self.current / self.total) * 100 if self.total > 0 else 0
            
            bar_len = 30
            filled_len = int(round(bar_len * self.current / float(self.total)))
            
            bar = 'â–ˆ' * filled_len + '-' * (bar_len - filled_len)
            
            # \r ile satÄ±r baÅŸÄ±na dÃ¶nerek aynÄ± satÄ±rÄ± gÃ¼ncelle
            sys.stdout.write(f'\r{self.desc}: |{bar}| {self.current}/{self.total} [{percent:.1f}%] @ {rate:.2f} items/s')
            sys.stdout.flush()

    def finish(self):
        sys.stdout.write('\n')

def find_crawl_archives(start_url):
    print(f"{CYAN}{BOLD}[*] FAZ 1: Ana arÅŸiv listesi taranÄ±yor...{ENDC}")
    session = create_optimized_session()
    try:
        response = session.get(start_url, timeout=CONNECT_TIMEOUT)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        crawl_links = soup.find_all('a', href=re.compile(r'CC-MAIN-'))
        urls = sorted(list(set([link['href'] for link in crawl_links])), reverse=True)
        print(f"{GREEN}[âœ“] FAZ 1 TamamlandÄ±: {len(urls)} arÅŸiv bulundu{ENDC}")
        return urls
    except Exception as e:
        print(f"{RED}[!] FAZ 1 HatasÄ±: {e}{ENDC}")
        return []

def process_single_archive(archive_url, progress):
    session = create_optimized_session()
    try:
        response = session.get(archive_url, timeout=CONNECT_TIMEOUT)
        soup = BeautifulSoup(response.text, 'html.parser')
        link = soup.find('a', href=re.compile(r'cc-index\.paths\.gz'))
        if not link: return []
        index_paths_url = urljoin(archive_url, link['href'])
        response_gz = session.get(index_paths_url, timeout=READ_TIMEOUT)
        decompressed = gzip.decompress(response_gz.content)
        paths = decompressed.decode('utf-8').strip().split('\n')
        tasks = []
        archive_name = archive_url.split('/')[-2]
        for path in paths:
            if not path: continue
            final_url = urljoin("https://data.commoncrawl.org/", path)
            save_path = os.path.join(OUTPUT_DIR, f"{archive_name}_{os.path.basename(path)}")
            tasks.append((final_url, save_path))
        return tasks
    except Exception:
        return []
    finally:
        if progress:
            progress.update()

def collect_all_download_tasks(archive_urls):
    print(f"\n{CYAN}{BOLD}[*] FAZ 2: CDX dosya listesi toplanÄ±yor...{ENDC}")
    all_tasks = []
    progress = SimpleProgress(len(archive_urls), "ArÅŸivler TaranÄ±yor")
    with ThreadPoolExecutor(max_workers=PARALLEL_ARCHIVE_SCAN) as executor:
        futures = {executor.submit(process_single_archive, url, progress): url for url in archive_urls}
        for future in as_completed(futures):
            tasks = future.result()
            all_tasks.extend(tasks)
    progress.finish()
    print(f"{GREEN}[âœ“] FAZ 2 TamamlandÄ±: {len(all_tasks):,} CDX dosyasÄ± bulundu{ENDC}")
    return all_tasks

def download_file_threaded(task, progress, stats):
    url, save_path = task
    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
        stats.increment('skipped')
        return
    session = create_optimized_session()
    try:
        with session.get(url, stream=True, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT), verify=False) as r:
            r.raise_for_status()
            temp_path = save_path + ".tmp"
            with open(temp_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=CHUNK_SIZE):
                    f.write(chunk)
            os.rename(temp_path, save_path)
        stats.increment('success')
    except Exception:
        if 'temp_path' in locals() and os.path.exists(temp_path):
            os.remove(temp_path)
        stats.increment('failed')
    finally:
        if progress:
            progress.update()

def download_all_files(tasks):
    print(f"\n{CYAN}{BOLD}[*] FAZ 3: HÄ±zlÄ± indirme baÅŸlÄ±yor...{ENDC}")
    print(f"{YELLOW}[*] {MAX_WORKERS} eÅŸzamanlÄ± asker gÃ¶revde...{ENDC}")
    
    stats = ThreadSafeCounter()
    progress = SimpleProgress(len(tasks), "CDX DosyalarÄ± Ä°ndiriliyor")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(download_file_threaded, task, progress, stats) for task in tasks}
        for future in as_completed(futures):
            try:
                future.result()
            except Exception:
                pass
    progress.finish()

    final_stats = stats.get_stats()
    print(f"\n{GREEN}{BOLD}[âœ“] FAZ 3 TamamlandÄ±{ENDC}")
    print(f"  {GREEN}âœ“ BaÅŸarÄ±lÄ±: {final_stats['success']}{ENDC}")
    print(f"  {YELLOW}âŠ˜ AtlandÄ±: {final_stats['skipped']}{ENDC}")
    print(f"  {RED}âœ— BaÅŸarÄ±sÄ±z: {final_stats['failed']}{ENDC}")

def main():
    print(f"{MAGENTA}{BOLD}")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ğŸš€ KRYPTON CDX EXTRACTOR (Python 3.6 Compatible)            â•‘")
    print("â•‘  âš¡ Massively Parallel | Resilient | Legacy Systems Ready      â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(ENDC)
    start_time = time.time()
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    try:
        archive_urls = find_crawl_archives(BASE_URL)
        if not archive_urls:
            return

        all_tasks = collect_all_download_tasks(archive_urls)
        if not all_tasks:
            return

        download_all_files(all_tasks)

    except KeyboardInterrupt:
        print(f"\n{YELLOW}{BOLD}[!] KULLANICI TARAFINDAN DURDURULDU{ENDC}")
    except Exception as e:
        print(f"\n{RED}{BOLD}[!] KRÄ°TÄ°K HATA: {e}{ENDC}")
    finally:
        elapsed = time.time() - start_time
        hours, rem = divmod(elapsed, 3600)
        minutes, seconds = divmod(rem, 60)
        print(f"\n{GREEN}{BOLD}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—{ENDC}")
        print(f"{GREEN}{BOLD}â•‘  âœ“ OPERASYON TAMAMLANDI                                      â•‘{ENDC}")
        print(f"{GREEN}{BOLD}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•{ENDC}")
        print(f"{CYAN}â±ï¸  Toplam SÃ¼re: {int(hours):02d}s {int(minutes):02d}d {int(seconds):02d}sn{ENDC}")
        print(f"{CYAN}ğŸ“ KayÄ±t Yeri: ./{OUTPUT_DIR}{ENDC}")

if __name__ == "__main__":
    main()
